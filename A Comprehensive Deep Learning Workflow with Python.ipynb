{
  "cells": [
    {
      "metadata": {
        "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384"
      },
      "cell_type": "markdown",
      "source": "## <div style=\"text-align: center\">A Comprehensive Deep Learning Workflow with Python </div>\n\n<div style=\"text-align: center\">This <b>tutorial</b> demonstrates the basic workflow  for <b>Deep Learning</b>.You should be familiar with basic <a href=\"https://www.kaggle.com/mjbahmani/linear-algebra-in-60-minutes\">linear algebra</a>,<a href=\"https://www.kaggle.com/mjbahmani/10-steps-to-become-a-data-scientist\">Python</a>\nand the Jupyter Notebook editor. It also helps if you have a basic understanding of <a href=\"https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python/\">Machine Learning</a>\n and classification.</div>\n<a href='https://github.com/mjbahmani/10-steps-to-become-a-data-scientist'><img src='http://s8.picofile.com/file/8341526350/deeplearning.png' width=400, height=400></a>\n<div style=\"text-align:center\">last update: <b>11/03/2018</b></div>\n\n\n\n>###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n---------------------------------------------------------------------\nFork and run my kernels on **GiHub**  and follow me:\n\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "cda11210a88d6484112cbe2c3624225328326c6a"
      },
      "cell_type": "markdown",
      "source": "## Notebook  Content\n1. [Introduction](#1)\n1. [Machine learning workflow](#2)\n1. [Problem Definition](#3)\n    1. [Problem feature](#4)\n    1. [Aim](#5)\n    1. [Variables](#6)\n1. [ Inputs & Outputs](#7)\n    1. [Inputs ](#8)\n    1. [Outputs](#9)\n1. [Installation](#10)\n    1. [ jupyter notebook](#11)\n    1. [ kaggle kernel](#12)\n    1. [Colab notebook](#13)\n    1. [install python & packages](#14)\n    1. [Loading Packages](#15)\n1. [Exploratory data analysis](#16)\n    1. [Data Collection](#17)\n    1. [Visualization](#18)\n    1. [Data Preprocessing](#30)\n1. [Python Deep Learning Packages](#31)\n    1. [Keras](#33)\n        1. [Analysis](#34)\n    1. [TensorFlow](#35)\n        1. [Import the Fashion MNIST dataset](#36)\n        1. [Explore the data](#37)\n        1. [Preprocess the data](#38)\n        1. [Build the model](#39)\n            1. [Setup the layers](#40)\n        1. [Compile the model](#41)\n        1. [Train the model](#42)\n        1. [Evaluate accuracy](#43)\n        1. [Make predictions](#44)\n    1. [Theano](#45)\n        1. [Theano( example)](#46)\n        1. [Calculating multiple results at once](#47)\n    1. [Pytroch](#48)\n        1. [Tensors](#49)\n    1. [Operations](#50)\n1. [Conclusion](#51)\n1. [References](#52)"
    },
    {
      "metadata": {
        "_uuid": "750903cc2679d39058f56df6c6c040be02b748df"
      },
      "cell_type": "markdown",
      "source": " <a id=\"1\"></a> <br>\n## 1- Introduction\nThis is a **comprehensive Deep Learning techniques with python**, it is clear that everyone in this community is familiar with **MNIST dataset** but if you need to review your information about the dataset please visit this [link](https://en.wikipedia.org/wiki/MNIST_database).\n\nI have tried to help  Kaggle users  how to face deep learning problems. and I think it is a great opportunity for who want to learn deep learning workflow with python completely.\n## 1-1 Courses\nThere are a lot of online courses that can help you develop your knowledge, here I have just  listed some of them:\n\n1. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https://www.coursera.org/specializations/deep-learning)\n1. [Deep Learning A-Z™: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/)\n\n1. [Creative Applications of Deep Learning with TensorFlow](https://www.class-central.com/course/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n1. [Neural Networks for Machine Learning](https://www.class-central.com/mooc/398/coursera-neural-networks-for-machine-learning)\n1. [Practical Deep Learning For Coders, Part 1](https://www.class-central.com/mooc/7887/practical-deep-learning-for-coders-part-1)\n\n## 1-2 Kaggle kernels\nI want to thanks **Kaggle team**  and  all of the **kernel's authors**  who develop this huge resources for Data scientists. I have learned from The work of others and I have just listed some more important kernels that inspired my work and I've used them in this kernel:\n\n1. [Deep Learning Tutorial for Beginners](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners)\n1. [introduction-to-cnn-keras-0-997-top-6](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n\n\n## 1-3 Ebooks\nSo you love reading , here is **10 free machine learning books**\n1. [Probability and Statistics for Programmers](http://www.greenteapress.com/thinkstats/)\n2. [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/091117.pdf)\n2. [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n2. [Understanding Machine Learning](http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)\n2. [A Programmer’s Guide to Data Mining](http://guidetodatamining.com/)\n2. [Mining of Massive Datasets](http://infolab.stanford.edu/~ullman/mmds/book.pdf)\n2. [A Brief Introduction to Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n2. [Deep Learning](http://www.deeplearningbook.org/)\n2. [Natural Language Processing with Python](https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python)\n2. [Machine Learning Yearning](http://www.mlyearning.org/)\n\n## 1-4 Cheat Sheets\nData Science is an ever-growing field, there are numerous tools & techniques to remember. It is not possible for anyone to remember all the functions, operations and formulas of each concept. That’s why we have cheat sheets. But there are a plethora of cheat sheets available out there, choosing the right cheat sheet is a tough task. So, I decided to write this article.\n\nHere I have selected the cheat sheets on the following criteria: comprehensiveness, clarity, and content [26]:\n1. [Quick Guide to learn Python for Data Science ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Science-in-Python.pdf)\n1. [Python for Data Science Cheat sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/beginners_python_cheat_sheet.pdf)\n1. [Python For Data Science Cheat Sheet NumPy](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Numpy_Python_Cheat_Sheet.pdf)\n1. [Exploratory Data Analysis in Python]()\n1. [Data Exploration using Pandas in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Data-Exploration-in-Python.pdf)\n1. [Data Visualisation in Python](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/data-visualisation-infographics1.jpg)\n1. [Python For Data Science Cheat Sheet Bokeh](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Python_Bokeh_Cheat_Sheet.pdf)\n1. [Cheat Sheet: Scikit Learn ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/Scikit-Learn-Infographic.pdf)\n1. [MLalgorithms CheatSheet](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/MLalgorithms-.pdf)\n1. [Probability Basics  Cheat Sheet ](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist/blob/master/cheatsheets/probability_cheatsheet.pdf)\n\n### 1-5 Deep Learning vs Machine Learning\nWe use a **machine algorithm** to parse data, learn from that data, and make informed decisions based on what it has learned. Basically, **Deep Learning** is used in layers to create an **Artificial Neural Network** that can learn and make intelligent decisions on its own. We can say **Deep Learning is a sub-field of Machine Learning**.\n\n<img src ='https://cdn-images-1.medium.com/max/800/1*ZX05x1xYgaVoa4Vn2kKS9g.png'>\n\n\nI am open to getting your feedback for improving this **kernel**\n"
    },
    {
      "metadata": {
        "_uuid": "e11b73b618b0f6e4335520ef80267c6d577d1ba5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n## 2- Deep Learning Workflow\nIf you have already read some [Deep Learning books](https://towardsdatascience.com/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into deep learning.\n\nmost of these books share the following steps:\n*   Define Problem\n*   Specify Inputs & Outputs\n*   Exploratory data analysis\n*   Data Collection\n*   Data Preprocessing\n*   Data Cleaning\n*   Visualization\n*   Model Design, Training, and Offline Evaluation\n*   Model Deployment, Online Evaluation, and Monitoring\n*   Model Maintenance, Diagnosis, and Retraining\n\n**You can see my workflow in the below image** :\n <img src=\"http://s9.picofile.com/file/8338227634/workflow.png\" />\n\n"
    },
    {
      "metadata": {
        "_uuid": "600be852c0d28e7c0c5ebb718904ab15a536342c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n## 3- Problem Definition\nI think one of the important things when you start a new deep learning project is Defining your problem.\n\nProblem Definition has four steps that have illustrated in the picture below:\n<img src=\"http://s8.picofile.com/file/8338227734/ProblemDefination.png\">\n<a id=\"4\"></a> <br>\n### 3-1 Problem Feature\nwe will use the classic MNIST  data set. This dataset contains information about  handwritten digits that is commonly used for training various image processing systems.\nhe MNIST database contains 60,000 training images and 10,000 testing images.\n\nHalf of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate\n<a id=\"5\"></a> <br>\n### 3-2 Aim\n your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images\n<a id=\"6\"></a> <br>\n### 3-3 Variables\nEach **pixel** column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n"
    },
    {
      "metadata": {
        "_uuid": "8bb4dfebb521f83543e1d45db3559216dad8f6fb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"7\"></a> <br>\n## 4- Inputs & Outputs\n<a id=\"8\"></a> <br>\n### 4-1 Inputs\nThe data files train.csv and test.csv contain gray-scale images of **hand-drawn digits**, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between **0 and 255**, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></img>\n<a id=\"9\"></a> <br>\n### 4-2 Outputs\nyour Output is to correctly identify digits from a dataset of tens of thousands of handwritten images."
    },
    {
      "metadata": {
        "_uuid": "89ee0cda57822cd4102eadf8992c5bfe1964d557"
      },
      "cell_type": "markdown",
      "source": "<a id=\"10\"></a> <br>\n## 5-Installation\n#### Windows:\n* Anaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n* Canopy (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/)\n#### Linux\nPackage managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n\nFor Ubuntu Users:\nsudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\npython-pandas python-sympy python-nose"
    },
    {
      "metadata": {
        "_uuid": "c1793fb141d3338bbc4300874be6ffa5cb1a9139"
      },
      "cell_type": "markdown",
      "source": "<a id=\"11\"></a> <br>\n## 5-1 Jupyter notebook\nI strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https://www.anaconda.com/download/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n\nFirst, download Anaconda. We recommend downloading Anaconda’s latest Python 3 version.\n\nSecond, install the version of Anaconda which you downloaded, following the instructions on the download page.\n\nCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):"
    },
    {
      "metadata": {
        "_uuid": "abbd1757dde9805758a2cec47a186e31dbc29822"
      },
      "cell_type": "markdown",
      "source": "> jupyter notebook\n> "
    },
    {
      "metadata": {
        "_uuid": "8a70c253d5afa93f07a7a7e048dbb2d7812c8d10"
      },
      "cell_type": "markdown",
      "source": "<a id=\"12\"></a> <br>\n## 5-2 Kaggle Kernel\nKaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al."
    },
    {
      "metadata": {
        "_uuid": "237bbe4e4509c9491ce165e3599c432b979d7b90"
      },
      "cell_type": "markdown",
      "source": "<a id=\"13\"></a> <br>\n## 5-3 Colab notebook\n**Colaboratory** is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use.\n### 5-3-1 What browsers are supported?\nColaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n### 5-3-2 Is it free to use?\nYes. Colaboratory is a research project that is free to use.\n### 5-3-3 What is the difference between Jupyter and Colaboratory?\nJupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser."
    },
    {
      "metadata": {
        "_uuid": "fbedcae8843986c2139f18dad4b5f313e6535ac5"
      },
      "cell_type": "markdown",
      "source": "<a id=\"15\"></a> <br>\n## 5-5 Loading Packages\nIn this kernel we are using the following packages:"
    },
    {
      "metadata": {
        "_uuid": "61f49281fdd8592b44c0867225f57e6fce36342c"
      },
      "cell_type": "markdown",
      "source": " <img src=\"http://s8.picofile.com/file/8338227868/packages.png\">\n"
    },
    {
      "metadata": {
        "_uuid": "5bf55263fff62fb1f9d478e0e11a4038a562637f"
      },
      "cell_type": "markdown",
      "source": "### 5-5-1 Import\n Now we import all of them "
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nimport tensorflow as tf\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport keras\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "351fe6ee8f49be819107ae6c7e6d7abddeef32f9"
      },
      "cell_type": "markdown",
      "source": "### 5-5-2 Version\nprint version of each package"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0fd91e925371ef73755d20d6232d0842206eb10"
      },
      "cell_type": "code",
      "source": "print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))\nprint('Keras: {}'.format(keras.__version__))\nprint('tensorflow: {}'.format(tf.__version__))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "187f8389fd6e034a5bb1555e3ed2fff5184a8f44"
      },
      "cell_type": "markdown",
      "source": "### 5-5-2 Setup\n\nA few tiny adjustments for better **code readability**"
    },
    {
      "metadata": {
        "_uuid": "cb7e4af0977f267f0055ef6c7b9d7081cbaeb889",
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.set(style='white', context='notebook', palette='deep')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\nmpl.style.use('ggplot')\nsns.set_style('white')\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "04ff1a533119d589baee777c21194a951168b0c7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"16\"></a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n \n* Which variables suggest interesting relationships?\n* Which observations are unusual?\n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">"
    },
    {
      "metadata": {
        "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
      },
      "cell_type": "markdown",
      "source": "<a id=\"17\"></a> <br>\n## 6-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n"
    },
    {
      "metadata": {
        "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# import Dataset to play with it\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nfrom keras.datasets import mnist\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
      },
      "cell_type": "markdown",
      "source": "**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
    },
    {
      "metadata": {
        "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
      },
      "cell_type": "markdown",
      "source": "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
    },
    {
      "metadata": {
        "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51",
        "trusted": true
      },
      "cell_type": "code",
      "source": "type(train_images)\ntype(train_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2"
      },
      "cell_type": "markdown",
      "source": "## 6-1-1 Statistical Summary\n1- Dimensions of the dataset.\n\n2- Peek at the data itself.\n\n3- Statistical summary of all attributes.\n\n4- Breakdown of the data by the class variable.[7]\n\nDon’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects."
    },
    {
      "metadata": {
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# shape\nprint(train_images.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4b45251be7be77333051fe738639104ae1005fa5",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# shape\nprint(test_images.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#columns*rows\ntrain_images.size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#columns*rows\ntest_images.size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "277e1998627d6a3ddeff4e913a6b8c3dc81dec96"
      },
      "cell_type": "markdown",
      "source": "\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\nYou should see 42000 instances and 785 attributes for train.csv"
    },
    {
      "metadata": {
        "_uuid": "95ee5e18f97bc410df1e54ac74e32cdff2b30755"
      },
      "cell_type": "markdown",
      "source": "for getting some information about the dataset you can use **info()** command"
    },
    {
      "metadata": {
        "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(train.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae08b544a8d4202c7d0a47ec83d685e81c91a66d"
      },
      "cell_type": "markdown",
      "source": "to check the first 5 rows of the data set, we can use head(5)."
    },
    {
      "metadata": {
        "_uuid": "5899889553c3416b27e93efceddb106eb71f5156",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train.head(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1150b6ac3d82562aefd5c64f9f01accee5eace4d"
      },
      "cell_type": "markdown",
      "source": "to check out last 5 row of the data set, we use tail() function"
    },
    {
      "metadata": {
        "_uuid": "79339442ff1f53ae1054d794337b9541295d3305",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train.tail() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c288c3dc8656a872a8529368812546e434d3a22"
      },
      "cell_type": "markdown",
      "source": "to pop up 5 random rows from the data set, we can use **sample(5)**  function"
    },
    {
      "metadata": {
        "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train.sample(5) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"
      },
      "cell_type": "markdown",
      "source": "to give a statistical summary about the dataset, we can use **describe() (but it is not useful for this dataset!)"
    },
    {
      "metadata": {
        "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train.describe() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6af5638e71e4f6d0bee777523245237744a48294"
      },
      "cell_type": "markdown",
      "source": "##  6-2 Data preparation"
    },
    {
      "metadata": {
        "_uuid": "c582e1c7e3ef4f5cf8557af9db786c4a51df1a50",
        "trusted": true
      },
      "cell_type": "code",
      "source": "Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train \n\ng = sns.countplot(Y_train)\n\nY_train.value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91dda1f631cf4ed362162501aaaac6d19cfd6cc7"
      },
      "cell_type": "markdown",
      "source": "<a id=\"30\"></a> <br>\n## 6-3 Data Preprocessing\n**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n \nData Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\nthere are plenty of steps for data preprocessing and we just listed some of them :\n* removing Target column (id)\n* Sampling (without replacement)\n* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n* Introducing missing values and treating them (replacing by average values)\n* Noise filtering\n* Data discretization\n* Normalization and standardization\n* PCA analysis\n* Feature selection (filter, embedded, wrapper)"
    },
    {
      "metadata": {
        "_uuid": "2d96fedf1a2fa6344990cb07b8d98d17a3a7b504"
      },
      "cell_type": "markdown",
      "source": "<a id=\"31\"></a> <br>\n# 7- Python Deep Learning Packages\n1. **keras**[11]\n>Well known for being minimalistic, the Keras neural network library (with a supporting interface of Python) supports both convolutional and recurrent networks that are capable of running on either TensorFlow or Theano. The library is written in Python and was developed keeping quick experimentation as its USP.\n1. **TensorFlow**\n> TensorFlow is arguably one of the best deep learning frameworks and has been adopted by several giants such as Airbus, Twitter, IBM, and others mainly due to its highly flexible system architecture.\n1. **Caffe**\n> Caffe is a deep learning framework that is supported with interfaces like C, C++, Python, and MATLAB as well as the command line interface. It is well known for its speed and transposability and its applicability in modeling convolution neural networks (CNN).\n1. **Microsoft Cognitive Toolkit/CNTK**\n> Popularly known for easy training and the combination of popular model types across servers, the Microsoft Cognitive Toolkit (previously known as CNTK) is an open-source deep learning framework to train deep learning models. It performs efficient convolution neural networks and training for image, speech, and text-based data. Similar to Caffe, it is supported by interfaces such as Python, C++, and the command line interface.\n1. **Torch/PyTorch**\n> Torch is a scientific computing framework that offers wide support for machine learning algorithms. It is a Lua-based deep learning framework and is used widely amongst industry giants such as Facebook, Twitter, and Google. It employs CUDA along with C/C++ libraries for processing and was basically made to scale the production of building models and provide overall flexibility.\n<img src='https://cdn-images-1.medium.com/max/800/1*dYjDEI0mLpsCOySKUuX1VA.png'>\n*State of open source deep learning frameworks in 2017*\n\n1. **MXNet**\n> Designed specifically for the purpose of high efficiency, productivity, and flexibility, MXNet(pronounced as mix-net) is a deep learning framework supported by Python, R, C++, and Julia.\n1. **Chainer**\n>Highly powerful, dynamic and intuitive, Chainer is a Python-based deep learning framework for neural networks that is designed by the run strategy. Compared to other frameworks that use the same strategy, you can modify the networks during runtime, allowing you to execute arbitrary control flow statements.\n1. **Deeplearning4j**\n>Parallel training through iterative reduce, microservice architecture adaptation, and distributed CPUs and GPUs are some of the salient features of the Deeplearning4j deep learning framework. It is developed in Java as well as Scala and supports other JVM languages, too.\n1. **Theano**\n>Theano is beautiful. Without Theano, we wouldn’t have anywhere near the amount of deep learning libraries (specifically in Python) that we do today. In the same way that without NumPy, we couldn’t have SciPy, scikit-learn, and scikit-image, the same can be said about Theano and higher-level abstractions of deep learning.\n1. **Lasagne**\n>Lasagne is a lightweight library used to construct and train networks in Theano. The key term here is lightweight — it is not meant to be a heavy wrapper around Theano like Keras is. While this leads to your code being more verbose, it does free you from any restraints, while still giving you modular building blocks based on Theano.\n1. **PaddlePaddle**\n>PaddlePaddle (PArallel Distributed Deep LEarning) is an easy-to-use, efficient, flexible and scalable deep learning platform, which is originally developed by Baidu scientists and engineers for the purpose of applying deep learning to many products at Baidu."
    },
    {
      "metadata": {
        "_uuid": "0560fa84a64ddeac53a4a10aaaa04a1424ad50f9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"32\"></a> <br>\n## 7-1 Comparison Frameworks\nHere is a comparison of most popular available tools:\n<img src='http://s8.picofile.com/file/8341687650/dl_com.png'>\nImage Source: Silicon Valley Data Science (SVDS.com)"
    },
    {
      "metadata": {
        "_uuid": "cc17ec452b22997c92948c59852e319960f8de48"
      },
      "cell_type": "markdown",
      "source": "<a id=\"33\"></a> <br>\n## 7-2 Keras\nOur workflow will be as follow[10]:\n1. first we will present our neural network with the training data, `train_images` and `train_labels`. \n1. The network will then learn to associate images and labels. \n1. Finally, we will ask the network to produce predictions for `test_images`, \n1. and we  will verify if these predictions match the labels from `test_labels`.\n\n**Let's build our network **"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "372dc6b32cdfdd8cdcec96ecd7589c6c2b8f857d"
      },
      "cell_type": "code",
      "source": "from keras import models\nfrom keras import layers\n\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb4af579abbb2d8de84c8799bbca2a9129166b75"
      },
      "cell_type": "markdown",
      "source": "<a id=\"34\"></a> <br>\n## 7-2-1 Analysis\nThe core building block of neural networks is the \"**layer**\", a data-processing module which you can conceive as a \"**filter**\" for data. Some  data comes in, and comes out in a more useful form. Precisely, layers extract _representations_ out of the data fed into them -- hopefully  representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive \"**data distillation**\". \nA deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters -- the \"layers\".\nHere our network consists of a sequence of two `Dense` layers, which are densely-connected (also called \"fully-connected\") neural layers. \nThe second (and last) layer is a 10-way \"**softmax**\" layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\nTo make our network ready for training, we need to pick three more things, as part of \"compilation\" step:\n\n1. A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction.\n1. An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n1. Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1c5dde77030c5917a6192452a9b27d93aafc04f"
      },
      "cell_type": "code",
      "source": "network.compile(optimizer='rmsprop',\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4512d98c72019135dd41f9f0521be6ece298be24"
      },
      "cell_type": "markdown",
      "source": "\nBefore training, we will **preprocess** our data by reshaping it into the shape that the network expects, and **scaling** it so that all values are in \nthe `[0, 1]` interval. Previously, our training images for instance were stored in an array of shape `(60000, 28, 28)` of type `uint8` with \nvalues in the `[0, 255]` interval. We transform it into a `float32` array of shape `(60000, 28 * 28)` with values between 0 and 1."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11ab3b8562444b8af7c342c36a45fefa20387e3b"
      },
      "cell_type": "code",
      "source": "train_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype('float32') / 255\n\ntest_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype('float32') / 255",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ff688de9ee4e5ec11da63bb2f3ef48a49d7b2dd7"
      },
      "cell_type": "markdown",
      "source": "We also need to **categorically encode** the labels"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc813e561b55b29d1e0ba7f620ae29bf43293b57"
      },
      "cell_type": "code",
      "source": "from keras.utils import to_categorical\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe77d9bdd7566b201e5be7fe54c67c7f9717c956"
      },
      "cell_type": "markdown",
      "source": "We are now ready to train our network, which in **Keras** is done via a call to the `fit` method of the network: \nwe \"fit\" the model to its training data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18dd966ba9e372810d2e6ef3950182a6fb91f779"
      },
      "cell_type": "code",
      "source": "network.fit(train_images, train_labels, epochs=5, batch_size=128)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7542f033cbc2c6a14b406319e822b2482600bc55"
      },
      "cell_type": "markdown",
      "source": "**Two quantities** are being displayed during training: the \"**loss**\" of the network over the training data, and the accuracy of the network over \nthe training data.\n\nWe quickly reach an accuracy of **0.989 (i.e. 98.9%)** on the training data. Now let's check that our model performs well on the test set too:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3ed834610a6f66f1e112a7c8288e84a0dd410b8"
      },
      "cell_type": "code",
      "source": "test_loss, test_acc = network.evaluate(test_images, test_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e00d4ca1dff83c73886044b8049d4fcc6930669"
      },
      "cell_type": "code",
      "source": "print('test_acc:', test_acc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac27321f82907d98f22b2ab6fee54a5885c7cbb1"
      },
      "cell_type": "markdown",
      "source": "\n**Our test set accuracy turns out to be 97.8%**"
    },
    {
      "metadata": {
        "_uuid": "2d318a6552c109d8880ce14659c90210eab060c0"
      },
      "cell_type": "markdown",
      "source": "<a id=\"35\"></a> <br>\n## 7-3 TensorFlow\n**TensorFlow** is an open-source machine learning library for research and production. TensorFlow offers **APIs** for beginners and experts to develop for desktop, mobile, web, and cloud. See the sections below to get started.[12]"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef0b5f6ef0e836220d916fe1ec65844654a350ff"
      },
      "cell_type": "code",
      "source": "# Simple hello world using TensorFlow\nhello = tf.constant('Hello, TensorFlow!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d152369cb2b86c4b03af2e5ce2b9fc3ee6f0717"
      },
      "cell_type": "code",
      "source": "# Start tf session\nsess = tf.Session()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba6e4b12ca3b80dd25068a0469843a0d00c49ff3"
      },
      "cell_type": "code",
      "source": "# Run graph\nprint(sess.run(hello))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e5f85da49eb80ae47151b5b3308425b9184e1440"
      },
      "cell_type": "code",
      "source": "mnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fbc9fbb67e838ea9ddf88845b688c8c1951f7512"
      },
      "cell_type": "markdown",
      "source": "<a id=\"36\"></a> <br>\n## 7-3-1 Import the Fashion MNIST dataset\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c85ca45a881ec8f85cf072cd8cc299e540e0b453"
      },
      "cell_type": "code",
      "source": "fashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06829a241c328ba053315979757694774808e3c0"
      },
      "cell_type": "markdown",
      "source": "Loading the dataset returns **four NumPy arrays**:\n\n1. The train_images and train_labels arrays are the training set—the data the model uses to learn.\n1. The model is tested against the test set, the test_images, and test_labels arrays.\n1. The images are 28x28 NumPy arrays, with pixel values ranging between 0 and 255.\n1. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:"
    },
    {
      "metadata": {
        "_uuid": "ed1e9b140ee497c89ac5213787c157d412385ac6"
      },
      "cell_type": "markdown",
      "source": "<img src='https://tensorflow.org/images/fashion-mnist-sprite.png'>"
    },
    {
      "metadata": {
        "_uuid": "302664d8f9878c0c8ddb2a64bc00cd6ed58d4766"
      },
      "cell_type": "markdown",
      "source": "Each image is **mapped** to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0801170ba61fca12c2d39f246bcb4ece5f78f5df"
      },
      "cell_type": "code",
      "source": "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f2136d7bc6f8ef021e4d7f6ee69203baf43faabd"
      },
      "cell_type": "markdown",
      "source": "<a id=\"37\"></a> <br>\n## 7-3-2 Explore the data\nLet's explore the format of the dataset before training the model. The following shows there are **60,000** images in the training set, with each image represented as 28 x 28 pixels:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f4dc17d2affe5849b9a4e8ae81d2c9ffa5739c9"
      },
      "cell_type": "code",
      "source": "train_images.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9cbf875d1bb38ca779bf1ea9d081887889b18206"
      },
      "cell_type": "markdown",
      "source": "Likewise, there are 60,000 labels in the training set:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dc01e76b4d061bacbfaca818dac4ac6f15c7a44"
      },
      "cell_type": "code",
      "source": "len(train_labels)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dac4b85e6bbe9c6909ccaad22dfba927d8a95fe4"
      },
      "cell_type": "markdown",
      "source": "Each label is an integer between 0 and 9:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b1060a16ba153a47f8f9576281934d570b76a2b"
      },
      "cell_type": "code",
      "source": "train_labels\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b9524c3d082ba3d1fe08fc5117350d3831720ce"
      },
      "cell_type": "markdown",
      "source": "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11acf0145fd4293e8944da17db96ef0abf176fe2"
      },
      "cell_type": "code",
      "source": "test_images.shape\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "10d203635805eb675a08a1aa6cac8a18198bddb0"
      },
      "cell_type": "markdown",
      "source": "And the test set contains 10,000 images labels:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa2d61fe145d2040832162e2ab1cb42edb7aa3d1"
      },
      "cell_type": "code",
      "source": "len(test_labels)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aba35c08f64f4879210aaf6c8954e361cc34a6fb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"38\"></a> <br>\n## 7-3-3 Preprocess the data\n"
    },
    {
      "metadata": {
        "_uuid": "3d2f80c0d183ef02499dfba9e1d91e98aa109ada"
      },
      "cell_type": "markdown",
      "source": "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45f0894f5fbb16cfb153ab368db499f5ca3bf898"
      },
      "cell_type": "code",
      "source": "plt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d9d3aa056608f5ba327ade5a012a5732aa588a58"
      },
      "cell_type": "markdown",
      "source": "We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, cast the datatype of the image components from an** integer to a float,** and divide by 255. Here's the function to preprocess the images:\n\nIt's important that the training set and the testing set are preprocessed in the same way:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00845f727dea68873dd406f5eb70f4f6b47f8109"
      },
      "cell_type": "code",
      "source": "train_images = train_images / 255.0\n\ntest_images = test_images / 255.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "986c4006f09b115a8406c20f56b471ff99a77b7b"
      },
      "cell_type": "markdown",
      "source": "Display the first 25 images from the training set and display the class name below each image. **Verify** that the data is in the correct format and we're ready to build and train the network."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "741d898b017fcbf4cda5f1db742cd9a0f88b4edd"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2097815c69672cd1b9f56448acdf437339f568c9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"39\"></a> <br>\n## 7-3-4 Build the model\n"
    },
    {
      "metadata": {
        "_uuid": "d7b5a268f8e1dc866e7a3fb3a2f6cca0e8523240"
      },
      "cell_type": "markdown",
      "source": "**Building the neural network requires configuring the layers of the model, then compiling the model.**\n<a id=\"40\"></a> <br>\n### 7-3-4-1 Setup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\n\nMost of deep learning consists of chaining together simple layers. Most layers, like tf.keras.layers.Dense, have parameters that are learned during training."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fb66496035a544a5e94106cb2b80e7bcb11d5d4"
      },
      "cell_type": "code",
      "source": "model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation=tf.nn.relu),\n    keras.layers.Dense(10, activation=tf.nn.softmax)\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "891b791c682f6512cb837e4c9cd3bb3f96f255f7"
      },
      "cell_type": "markdown",
      "source": "The **first layer** in this network, tf.keras.layers.Flatten, transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\n\nAfter the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely-connected, or fully-connected, neural layers. The first Dense layer has 128 nodes (or neurons). **The second (and last) layer** is a 10-node softmax layer—this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes."
    },
    {
      "metadata": {
        "_uuid": "b8288c8b9365aaea1be7e3fc29f28ea099599337"
      },
      "cell_type": "markdown",
      "source": "<a id=\"41\"></a> <br>\n## 7-3-5 Compile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n\n1. **Loss function** —This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n1. **Optimizer** —This is how the model is updated based on the data it sees and its loss function.\n1. **Metrics** —Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "147f6384ab4ac6ca1f4199c9e5fe5fc5de40ddcf"
      },
      "cell_type": "code",
      "source": "model.compile(optimizer=tf.train.AdamOptimizer(), \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb5ca06622460737f9270c0f8b44e4a9ec8ee46c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"42\"></a> <br>\n## 7-3-6 Train the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model—in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set—in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\nTo start training, call the model.fit method—the model is \"fit\" to the training data:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03987dc45cf8603ac85093b2e213b66414997307"
      },
      "cell_type": "code",
      "source": "model.fit(train_images, train_labels, epochs=5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1ac81e58e42d648d321b830b224ab3519984ebf3"
      },
      "cell_type": "markdown",
      "source": "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.88 (or 88%) on the training data."
    },
    {
      "metadata": {
        "_uuid": "87b748f0572109246d2867d3cbd0099279a8e14b"
      },
      "cell_type": "markdown",
      "source": "<a id=\"43\"></a> <br>\n## 7-3-7 Evaluate accuracy\nNext, compare how the model performs on the test dataset:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b058c57a1019bfe9eb5bb130821521b8fb60199"
      },
      "cell_type": "code",
      "source": "test_loss, test_acc = model.evaluate(test_images, test_labels)\n\nprint('Test accuracy:', test_acc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e51ac16e784f873d188f3a7a9a97b77b80bc9fd"
      },
      "cell_type": "markdown",
      "source": "It turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data."
    },
    {
      "metadata": {
        "_uuid": "89b4451f22b40620f84d91776bc87f56c7035b56"
      },
      "cell_type": "markdown",
      "source": "<a id=\"44\"></a> <br>\n## 7-3-8 Make predictions\nWith the model trained, we can use it to make predictions about some images."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41c87068a6d3a43561ea86165f0a06a2118a62d2"
      },
      "cell_type": "code",
      "source": "predictions = model.predict(test_images)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c67111925e00718f02853fa0a2e075e57ca11a9"
      },
      "cell_type": "markdown",
      "source": "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b55b3fd88dcc23ad88449a3e4031c8cafb898eb0"
      },
      "cell_type": "code",
      "source": "predictions[0]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "133bc34cbb4d3f253c524f00e222928a3b13e4b4"
      },
      "cell_type": "markdown",
      "source": "A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5d06c4e9c154f2a5b104fb40950caea662866928"
      },
      "cell_type": "code",
      "source": "np.argmax(predictions[0])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "416bd6ab38af768e7ccbf84f2e4958edf57607a7"
      },
      "cell_type": "markdown",
      "source": "We can graph this to look at the full set of 10 channels\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f47aca692a640ad3b80f8866d6008aba68ed0825"
      },
      "cell_type": "code",
      "source": "def plot_image(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  \n  plt.imshow(img, cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n  \n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  predictions_array, true_label = predictions_array[i], true_label[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1]) \n  predicted_label = np.argmax(predictions_array)\n \n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label].set_color('blue')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "659d69546cc3ba435b279de4b3672a73b4c188c2"
      },
      "cell_type": "markdown",
      "source": "Let's look at the 0th image, predictions, and prediction array.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6567c22e4a9e6f5bb08971148ec301d7851293c1"
      },
      "cell_type": "code",
      "source": "i = 0\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions,  test_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d38e7cb0dafada9806aa9e97ea96b77f64c43da"
      },
      "cell_type": "code",
      "source": "i = 12\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions,  test_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb5e128f74d125f210335318f1caad4d9d57c93c"
      },
      "cell_type": "markdown",
      "source": "Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1bfd76dfc22dc76ccc17eaa2209144cb0571b0a4"
      },
      "cell_type": "code",
      "source": "# Plot the first X test images, their predicted label, and the true label\n# Color correct predictions in blue, incorrect predictions in red\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n  plot_image(i, predictions, test_labels, test_images)\n  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n  plot_value_array(i, predictions, test_labels)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aad0da875f601a4834cb339909d54f5e2c4a053f"
      },
      "cell_type": "markdown",
      "source": "Finally, use the trained model to make a **prediction** about a single image.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "afed032300abedc76b58a2356723ab6287873ded"
      },
      "cell_type": "code",
      "source": "# Grab an image from the test dataset\nimg = test_images[0]\n\nprint(img.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "488bcd1e9545884cef77acca23e278b337a5b39a"
      },
      "cell_type": "markdown",
      "source": "tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc0ad2cca369582a07a54298480d94c9e5505728"
      },
      "cell_type": "code",
      "source": "# Add the image to a batch where it's the only member.\nimg = (np.expand_dims(img,0))\n\nprint(img.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "013ac335e5e6815afed7467b8085d8900dfab7f2"
      },
      "cell_type": "markdown",
      "source": "Now predict the image:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49752739055f2c89268f0d7f08b2082759b9d1c4"
      },
      "cell_type": "code",
      "source": "predictions_single = model.predict(img)\n\nprint(predictions_single)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29f7bf38561a2e2fe1cfa1e9b3d578d47a9bb225"
      },
      "cell_type": "code",
      "source": "plot_value_array(0, predictions_single, test_labels)\n_ = plt.xticks(range(10), class_names, rotation=45)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cb26de113c9d2c8728a1bee7086638d29578d144"
      },
      "cell_type": "markdown",
      "source": "<a id=\"45\"></a> <br>\n# 7-4 Theano \nTheano is a numerical computation library for Python. It is a common choice for implementing neural network models as it allows you to efficiently define, optimize and evaluate mathematical expressions, including multi-dimensional arrays (numpy.ndaray).[13]\n"
    },
    {
      "metadata": {
        "_uuid": "46237310b57b3d39536d6ab2efc3ca2ebf01afd7"
      },
      "cell_type": "markdown",
      "source": "Theano has got an amazing compiler which can do various optimizations of varying complexity. A few of such optimizations are:\n\n1. Arithmetic simplification (e.g: --x -> x; x + y - x -> y)\n1. Using memory aliasing to avoid calculation\n1. Constant folding\n1. Merging similar subgraphs, to avoid redundant calculation\n1. Loop fusion for elementwise sub-expressions\n1. GPU computations"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8dbf39c20e9dacae59144b638c678656fd0b9019"
      },
      "cell_type": "code",
      "source": "import theano\nfrom theano import tensor\n\nx = tensor.dscalar()\ny = tensor.dscalar()\n\nz = x + y\nf = theano.function([x,y], z)\nprint(f(1.5, 2.5))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2fcb791936664cf4a967fef79d0cdf8bbd418af1"
      },
      "cell_type": "markdown",
      "source": "<a id=\"46\"></a> <br>\n## 7-4-1 Theano( example)"
    },
    {
      "metadata": {
        "_uuid": "fdbc14bdda90fbb00590a05d6226f48f234ef2e6"
      },
      "cell_type": "markdown",
      "source": "Let’s have a look at rather more elaborate example than just adding two numbers. Let’s try to compute the logistic curve, which is given by:"
    },
    {
      "metadata": {
        "_uuid": "55ce83795011853f33834e9f511cd621bf8e2dcf"
      },
      "cell_type": "markdown",
      "source": "<img src='https://cdn.journaldev.com/wp-content/uploads/2018/01/logistic-curve.png'>"
    },
    {
      "metadata": {
        "_uuid": "a327f3d9d841defc3cc8e4405d813eea6eb23517"
      },
      "cell_type": "markdown",
      "source": "If we plot a graph for this equation, it will look like:\n"
    },
    {
      "metadata": {
        "_uuid": "67a46003e16d368f8d70f236fbed7457c7377bf6"
      },
      "cell_type": "markdown",
      "source": "<img src='https://cdn.journaldev.com/wp-content/uploads/2018/01/logistic-curve-1.png'>"
    },
    {
      "metadata": {
        "_uuid": "bdae0bbac7a40ee20481147a352ec92d7e9e4c28"
      },
      "cell_type": "markdown",
      "source": "Logistic function is applied to each element of matrix. Let’s write a code snippet to demonstrate this:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc2bfb69b0c193493ef561d82e3228e2dae2db83"
      },
      "cell_type": "code",
      "source": "# declare a variable\nx = tensor.dmatrix('x')\n\n# create the expression\ns = 1 / (1 + tensor.exp(-x))\n\n# convert the expression into a callable object which takes\n# a matrix as parameter and returns s(x)\nlogistic = theano.function([x], s)\n\n# call the function with a test matrix and print the result\nprint(logistic([[0, 1], [-1, -2]]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "21b7f9353ef201f9c93af63020898209513c46c4"
      },
      "cell_type": "markdown",
      "source": "<a id=\"47\"></a> <br>\n## 7-4-2 Calculating multiple results at once\nLet’s say we have to compute elementwise difference, absolute difference and difference squared between two matrices ‘x’ and ‘y’. Doing this at same time optimizes program with significant duration as we don’t have to go to each element again and again for each operation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31737f220690dc8b8ab0106cb548af479696a3f3"
      },
      "cell_type": "code",
      "source": "# declare variables\nx, y = tensor.dmatrices('x', 'y')\n\n# create simple expression for each operation\ndiff = x - y\n\nabs_diff = abs(diff)\ndiff_squared = diff**2\n\n# convert the expression into callable object\nf = theano.function([x, y], [diff, abs_diff, diff_squared])\n\n# call the function and store the result in a variable\nresult= f([[1, 1], [1, 1]], [[0, 1], [2, 3]])\n\n# format print for readability\nprint('Difference: ')\nprint(result[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2a2e5fa1faaffea49f7e57d3ba1d56ce4ff659d9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"48\"></a> <br>\n## 7-5 Pytroch"
    },
    {
      "metadata": {
        "_uuid": "210f3ecc8793a0f5c5d6f05c63d1cf1f443c194f"
      },
      "cell_type": "markdown",
      "source": "It’s a Python-based scientific computing package targeted at two sets of audiences:\n\n1. A replacement for NumPy to use the power of GPUs\n1. A deep learning research platform that provides maximum flexibility and speed\n<img src='https://cdn-images-1.medium.com/max/800/1*5PLIVNA5fIqEC8-kZ260KQ.gif'>\n*PyTorch dynamic computational graph — source: http://pytorch.org/about/*"
    },
    {
      "metadata": {
        "_uuid": "fb4dee7a83d3fd5c6528b6acb6deb3ede07917c9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"49\"></a> <br>\n## 7-5-1 Tensors\nTensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9fa542c5e81368bf67ba10968825778888cb9ed"
      },
      "cell_type": "code",
      "source": "from __future__ import print_function\nimport torch",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "428a61e2f48669f3eda674ed89cadd52983d8f07"
      },
      "cell_type": "markdown",
      "source": "Construct a 5x3 matrix, uninitialized:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1367d89d0910e7ebbb474dd1f857f83b4ff4aece"
      },
      "cell_type": "code",
      "source": "x = torch.empty(5, 3)\nprint(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "984addcd6041b2b34506522bccea8efd3f27bf1e"
      },
      "cell_type": "markdown",
      "source": "Construct a randomly initialized matrix:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4410ff53288dfc7790cc40eb209df552d35487d6"
      },
      "cell_type": "code",
      "source": "x = torch.rand(5, 3)\nprint(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e0023c417e35ebeabf4bbcd9a3801fd381de4407"
      },
      "cell_type": "markdown",
      "source": "Construct a matrix filled zeros and of dtype long:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "827868be4fe401a9651ffc9bf95d37812dacb478"
      },
      "cell_type": "code",
      "source": "x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40eaebdbaa6cbf7f36e77891789bc1c05274bfb3"
      },
      "cell_type": "markdown",
      "source": "Construct a tensor directly from data:\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "21956cadded8ec22cb21b2d02ea99132fd016e6a"
      },
      "cell_type": "code",
      "source": "x = torch.tensor([5.5, 3])\nprint(x)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "885dbbc02d2e452f03c059e52303c6157064134d"
      },
      "cell_type": "markdown",
      "source": "or create a tensor based on an existing tensor. These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a649ed5e927a0d723397fc89627bb79ff777e221"
      },
      "cell_type": "code",
      "source": "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)    # override dtype!\nprint(x)                                      # result has the same size",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bab1c6447f5062d5a931c5e642b0f409d472fda7"
      },
      "cell_type": "markdown",
      "source": "Get its size:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62961e53b2bb3f99762d659902f10ef6a3c97332"
      },
      "cell_type": "code",
      "source": "print(x.size())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a6e5752f4585ad9ae0e0592c33c56b1675626cb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"50\"></a> <br>\n## 7-5-2 Operations\nThere are multiple syntaxes for operations. In the following example, we will take a look at the addition operation.\n\nAddition: syntax 1"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aeffc489a8e0c446c7c8532054e1892021b1bfcd"
      },
      "cell_type": "code",
      "source": "y = torch.rand(5, 3)\nprint(x + y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "92344324d330a3adbe7ac2c4f612fb55953249b2"
      },
      "cell_type": "markdown",
      "source": "Addition: syntax 2\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75b4d2d34207835bd33382894e86416cb7257dbe"
      },
      "cell_type": "code",
      "source": "print(torch.add(x, y))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f062bcc0e8a5d5c927ad8cde707bced9c569054d"
      },
      "cell_type": "markdown",
      "source": "Addition: providing an output **tensor** as argument\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65caf77ef11dde7550d2ee7b19f9acec6d101840"
      },
      "cell_type": "code",
      "source": "result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"
      },
      "cell_type": "markdown",
      "source": "you can follow and fork my work  in **GitHub**:\n> ###### [ GitHub](https://github.com/mjbahmani)\n\n\n--------------------------------------\n\n **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n "
    },
    {
      "metadata": {
        "_uuid": "0cb16fa384dea2e2f02cc2169be3f2eed37fa3fd"
      },
      "cell_type": "markdown",
      "source": "<a id=\"51\"></a> <br>\n# 8- Conclusion\nIn this kernel we have just tried to create a comprehensive deep learning workflow for helping you to  start your jounery in DL.\nsurly it is not completed yet!! also I want to hear your voice to improve kernel together."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<a id=\"52\"></a> <br>\n\n-----------\n\n# 9- References\n1. [https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n1. [keras](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n1. [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n1. [Sklearn](http://scikit-learn.org/)\n1. [machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n1. [Data Cleaning](http://wp.sigmod.org/?p=2288)\n1. [Kaggle kernel that I use it](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)\n1. [DL vs ML](https://medium.com/swlh/ill-tell-you-why-deep-learning-is-so-popular-and-in-demand-5aca72628780)\n1. [neural-networks-deep-learning](https://www.coursera.org/learn/neural-networks-deep-learning)\n1. [deep-learning-with-python-notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)\n1. [8-best-deep-learning-frameworks-for-data-science-enthusiasts](https://medium.com/the-mission/8-best-deep-learning-frameworks-for-data-science-enthusiasts-d72714157761)\n1. [tensorflow](https://www.tensorflow.org/tutorials/keras/basic_classification)\n1. [Theano](https://www.journaldev.com/17840/theano-python-tutorial)\n1. [pytorch](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n\n\n-------------\n"
    },
    {
      "metadata": {
        "_uuid": "7ca5f2ba5afd30d1a741453958a1337af35be208"
      },
      "cell_type": "markdown",
      "source": "### New Chapter Coming Soon, it is not completed"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}